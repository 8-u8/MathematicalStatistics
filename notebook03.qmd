[TOP](https://8-u8.github.io/MathematicalStatistics/)

# 数理統計学ノート03 多次元確率分布
1変量の分布は一旦すっ飛ばした(僕が担当じゃないので)。  
多次元の確率分布について記述する。

## 同時分布と条件付き分布
$X$と$Y$を2つの確率変数とおく。この2つの確率変数の組$(X, Y)$を確率変数ベクトルと呼び、
$(X, Y)$が同時に発生する事象を考える。  
たとえば10円玉のコイントスの実現値を$X$、100円玉のコイントスの実現値を$Y$として、  
両手で同時にコイントスをする事を考えるのも良い。このとき表が出たら1、裏が出たら0と定義すれば、
確率変数は実際に$\mathbb{R}^2$(これは二次元の実数の集合を意味する)に属する要素の上で定義される。

### 同時分布と周辺分布の理論
同時分布について、まず離散確率変数を考える。  
基本的には単変量の分布を考えることと同様に考えることができる。  
「同時」の名前は、$(X,Y)$に関する具体的な値が一緒に与えられることを意味する。  
このとき、同時確率は
$$
p(x, y) = P(X = x, Y = y)
$$
で与えられるし、累積分布関数は
$$
F(x, y) = P(X \leq x, Y \leq y)
$$
で定義できる。基本は1次元確率分布と同様に定義できる。  
他の本でも基本は同様であるか[[久保川]](https://amzn.asia/d/eRgNHzehttps://amzn.asia/d/eRgNHze)、最初から一般の$d$次元確率変数の分布について定義をして、
$d=1$や$d=2$のときを特別な場合として記述しているような書籍もある[[清水]](https://8-u8.github.io/MathematicalStatistics/notebook03.html)。

$X$の周辺分布は、同時に起こりうる$Y$の値すべてを考慮したときの確率で、  
以下のように表現される。

$$
F_X(x) = P(X \leq x, Y \leq \infty)=\sum_{x \in X}^x\sum_{y \in Y}^{\infty}p(x, y)
$$

### クロス表と同時分布

もっと気楽に考えるとクロス集計表は同時分布の離散表現に近い。  
行和/列和がそれぞれ<s>同時</s>周辺分布である。これがわかるとまあまあ集計時の見通しが良くなると思うし、  
クロス集計表が同時分布であるという雰囲気を感じるだけで「なんでこの表で検定ができるんだ？」  
というところにふんわりとした直感が働くようになるとも思う。

### 同時分布と周辺分布の可視化
ChatGPTに「同時分布のようなグラフを描くRのコードを描いて！」ってお願いしてみる。  
出てきた結果の一部は見にくい書き方をしていたので、ちょっと勝手に添削をしています。  

```{r}
# define density mixture function
dens_mixture <- function(x, y, mu = 0, sigma = 1) {
    out <- dnorm(x, mean = mu, sd = sigma) * dnorm(y, mean = mu, sd = sigma)
    out
}
# code below was written by ChatGPT
# Create a grid of x and y values
x <- seq(-4, 4, length.out = 100)
y <- seq(-4, 4, length.out = 100)
z <- outer(x, y, dens_mixture)

# Create the 3D plot
persp(x, y, z,
    a = 30, phi = 30, expand = 0.5,
    col = "lightblue", ltheta = 120, shade = 0.3,
    xlab = "X", ylab = "Y", zlab = "probability"
)

```

周辺分布は、例えば以下。上と右にあるのが周辺分布。  
ChatGPTに丸投げしたらあんまりいい感じにならなかったので、割と自分で書き直した。

```{r}
# Load the necessary libraries
set.seed(523)
library(ggplot2)
library(ggExtra)

# define density mixture function
dens_mixture <- function(x, y, mu = 0, sigma = 1) {
    out <- dnorm(x, mean = mu, sd = sigma) * dnorm(y, mean = mu, sd = sigma)
    out
}
# Define the normal distribution parameters
mu <- 0
sigma <- 1

# Create a dataset with x and y values
x <- rnorm(1000, mu, sigma)
y <- rnorm(1000, mu, sigma)
df <- data.frame(x = x, y = y)
# Create the 2D plot with marginal distributions
p <- ggplot2::ggplot(df) +
    ggplot2::geom_density2d(aes(x = x, y = y), geom = "line") +
    ggplot2::geom_point(aes(x = x, y = y)) +
    ggplot2::scale_fill_gradient(low = "white", high = "blue")

ggExtra::ggMarginal(p, type = "density")


```



### 条件付き分布

条件付き分布は、$(X, Y)$のうち、どちらか一方が与えられたという状態で、  
もう一方が実現する確率を表現する。つまり「同時」ではないし、  
お互いが何らかの法則を持って連動している。

$X=x$が与えられたときの$Y$の条件付き確率は以下のように記述される。

$$
P(Y=y|X=x) = \frac{P(X=x, Y=y)}{P(X=x)} = \frac{P(X=x, Y=y)}{\sum_{y \in Y}p(x, y)}
$$

あるいは以下のほうが使いやすいかも

$$
P(X=x, Y=y) = P(Y=y|X=x)P(X=x)
$$

もっというと、ここからベイズの定理が出てくる

$$
P(Y=y|X=x) = \frac{P(X=x|Y=y)P(Y=y)}{P(X=x)}
$$


解釈すると、同時分布を周辺分布で割る、ということになる。  
2値変数の同時分布は、以下のクロス表で表現できる。  
いつも見るクロス表だと、表側(行)が主語、表頭(列)が述語になるので、  
「Y=0の場合は、X=0になる確率は」という解釈をすることもなくはないが、  
これがクロス表で妥当な解釈であると考えられる場合は、後述の独立性を  
仮定として受け入れられるとき。  

|    | X=0|X=1 |  Yの周辺分布  |
|----|----|----|----|
|Y=0 |$P(X=0,Y=0)$|$P(X=1,Y=0)$|$P(Y=0)$|
|Y=1 |$P(X=0,Y=1)$|$P(X=1,Y=1)$|$P(Y=1)$|
|Xの周辺分布  |$P(X=0)$|$P(X=1)$|$1$|

$X$と$Y$が互いに独立であるとは、お互いの条件付き確率がもう一方に依存しないこと。  
つまり、上記の掛け算の方の式から、$P(Y=y|X=x) = P(Y=y)$が保証されること。
$$
P(X=x, Y=y) = P(Y=y)P(X=x) 
$$

ここまでなんとなく離散でやってきたけど基本的には連続化するアプローチは  
そんなに不自然で理解の難しい内容ではない。微小な定数$\Delta x$を用いて、  
0に向けて極限をとることで表現がなされる。  
周辺分布は興味のない方を積分すれば得られるし、同時分布は重積分で得られる。  

連続値における条件付き密度関数は、測度論的には注意が必要らしいけど、  
一旦踏み込むのを辞める。

> この条件付き密度関数の定義は直観的には正しいものであるが、測度論的には実はより  
> 注意深い議論が必要である。それは連続分布について最初に述べたように$X$が連続な  
> 確率変数ならば任意の$x$について$X=x$という事象の確率は0であるから、条件付き密度関数が
> どのような意味で定義できるかが明らかではないからである。

### 条件付き独立
条件付き独立はよく因果推論、及び因果探索で議論される事が多い気がする。  
3つの確率変数$X$、$Y$、$Z$があり、$Z=z$が与えられたときに、$X$と$Y$が条件付き独立であるとは、
$$
f_{X,Y|Z=z}(X,Y) = f_{X|Z=z}(x)f_{Y|Z=z}(y)
$$

を満たすことをいう。

### Todo: 条件付き独立の応用場面について
ごめん一旦ヤコビアン先でいい？

## ヤコビアンと畳み込み
多次元の連続確率変数を扱うとき、確率変数の変換と、それに連動する(事がある)  
密度関数の変換が重要なテクニックとなってくる。  
テクニックの大本は、多重積分における変数変換なので基本的には数学。  
わかる人にはわかるだろうが僕にはわからないので結果の整理だけでも竹村準拠でやっていく。  
が、実際にヤコビアンで解く例はちょっとわかりにくいので、他の本を参照する。    
なお、ヤコビアンの理解には以下の書籍も参考にしている

- [永田靖『統計学のための数学入門30講』](https://www.asakura.co.jp/detail.php?book_code=11633)
  - まあ初手ここだよね。一応事実のみが書かれている
- [加藤文元『数件講座シリーズ 大学教養微分積分』](https://www.chart.co.jp/goods/item/sugaku/39940.php)
  - しっかり数学のテクニックを学ぶなら、最近の良い参考書だと思う

### 準備
$X = (X_1,X_2, ..., X_n)$を$n$次元の確率**変数**ベクトルとし(竹村本では「確率ベクトル」)、  
その同時密度関数を$f_X(x)$とする。この確率変数ベクトルに変換$g$を噛ませて、その結果を$Y$、  
すなわち$Y = g(X) = (g_1(x), g_2(x), ..., g_n(x))$として考える。  
この$g$にはいくつか設定がある。

- すべての$i=1,2,...,n$に対して$g_i(x)$は連続微分可能
- すべての$x \in X$に対し、$g(X)$が1対1対応
- 連続微分可能な逆関数$g^{-1}(y)$が存在する

逆関数の存在条件とその証明については一旦議論を省く。

解きたい問題は、$X$の密度関数がわかっているとき、どうやって$Y$の分布関数を求めようか？ということ。  
ここでヤコビアンが必要になるらしい。

### ヤコビ行列
自然言語でいえば、「$x$の各要素を$y$の各要素で偏微分したときの偏微分係数を成分にした行列」で、  
提案者の Carl Gustav Jacob **J**acobi にちなんだ$J$で表現されることが多い。
$x = g^{-1}(y) = h(y)$と置いたときに、  

\begin{equation}
J(\frac{\partial{x}}{\partial{y}})= 

\left(
    \begin{matrix}
        \frac{\partial{h_1(y)}}{\partial{y_1}} & ... &
        \frac{\partial{h_n(y)}}{\partial{y_1}} \\
        \vdots & \ddots & \vdots \\
        \frac{\partial{h_1(y)}}{\partial{y_n}} & ... &
        \frac{\partial{h_n(y)}}{\partial{y_n}} \\
    \end{matrix}
\right)

\end{equation}

というような行列$J$をヤコビ行列(あるいは関数行列)と呼ぶことが多い。  

ヤコビ行列の特殊なパターンが勾配ベクトルという考え方は別段間違いではない。  
多変数実数関数$f(x_1, x_2, ..., x_n)$を各$x_i$で微分したものを並べたベクトルなので、  
まあまあ近いものとして捉えて良いと思う。

参考: [ヤコビ行列？勾配(grad)？](https://for-spring.com/analysis/partialdifferentiation-4/#toc4)

### ヤコビアン
ヤコビアンとして表現されるのはヤコビ行列の行列式のことである。  
統計学だと時々「ヘッシアン」とか出てくるが、これもヘッセ行列の行列式なので、  
そういうものだと思ってよい。でもハミルトニアンやラプラシアン、ラグランジアンとかも出てくるが、  
こっちは別に行列式ではなさそう。

ヤコビ行列の行列式を$det(J)$とおくと、$Y$の密度関数は

$$
f_Y(y) = f_X(g^{-1}(y))|det(\frac{\partial{x}}{\partial{y}})| = \\
f_X(x)|\frac{\partial{x}}{\partial{y}}|
$$

と書ける。ややこしい書き方をしているが「元の確率変数の密度関数になんぼか重みをつけてスケールを揃えよう」という雰囲気を感じてくれれば、そこまでお気持ちとして理解不能な変換ではないように思う。

問題はこの変換によってえられた密度関数$f_Y(y)$がちゃんと確率密度関数になっているんだろうか？  
というところだが、どうやら大丈夫そう(3.21式と3.22式との間)。

### 畳み込み
畳み込みによる確率分布の導出は統計検定だとだいぶお作法と化している事が多い。  
竹村本は割と回りくどいことをしているので、以下は久保川などの流儀も混ぜておく。  
$X$と$Y$を、密度関数が$f_X(x)$、$g_Y(y)$で与えられる独立な確率変数とする。  
$Z = X + Y$という変数変換を行ったときの密度関数を$f$と$g$で表現することを考える。  
$W = X$とおいて、変換を整理すると

\begin{equation}
\left\{\, 
    \begin{aligned}
        X = W \\
        Y = Z - W
    \end{aligned}
\right .
\end{equation}

と変換できる(逆関数)。ヤコビ行列を計算すると

\begin{equation}

\left(
    \begin{matrix}
        \frac{\partial{x}}{\partial{w}} & \frac{\partial{x}}{\partial{z}} \\
        \frac{\partial{y}}{\partial{w}} & \frac{\partial{y}}{\partial{z}} 
    \end{matrix}
\right)
= 
\left(
    \begin{matrix}
        1 & 0 \\
        -1 & 1 
    \end{matrix}
\right)
\end{equation}
で、ヤコビアンが1になる。ここが、嬉しい。

よって、$W$と$Z$の同時分布が$f(w)g(z-w)(×1)$ということがわかる(独立)ので、  
あとはこれを$w$について積分すれば、$z$の密度関数を算出できる。

もちろん、非線形な変換を与えたりすると、ヤコビアンは1にはならないので、  
ここに重みが加わったりする。

これの良いところは、分布関数がわかっていれば積分で計算ができること。  
$X$と$Y$がそれぞれ独立であれば、同じ分布でなくても問題はない。  
正規分布の再生性などもこれで証明ができる。  
これ以外の方法には特性関数を使う方法があるが、特性関数が明示的に表現できない分布には使えないこともある。

### 正規化定数の算出
ガンマ分布とベータ分布は次回でお願い。

#### (標準)正規分布
正規分布の正規化定数は$1 / \sqrt(2\pi)$として表されるが、  
「どこから$\pi$が出てきたんだ！？」と思うことが多い。  
ここではとりあえず導出をするが、なんで極座標表示が有効であるのかは未だに謎。

標準正規分布の密度関数はもう覚えていると思うが以下である。

$$
f(x) = \frac{1}{\sqrt{2\pi}}\exp{\{-\frac{x^2}{2}\}}
$$

この$\frac{1}{\sqrt{2\pi}}$を導出する。

$\frac{1}{c} = \int \exp{\{ - \frac{x^2}{2}\}}dx$とおいて、$X$と$Y$がそれぞれ独立に
$c\exp{\{ - \frac{x^2}{2}\}}$に従う確率変数とする。テクニックとしては極座標変換を行う。  
すなわち$(X, Y)$を以下の通りに極座標表示し、$(r, \theta)$の同時密度関数を求める。

\begin{equation}
\left\{\, 
    \begin{aligned}
        X = r \cos{\theta} \\
        Y = r \sin{\theta}
    \end{aligned}
\right .
\end{equation}


すると、ヤコビ行列とヤコビアンをこんな感じで計算ができる。

\begin{equation}
% ヤコビ行列
    \left(
        \begin{matrix}
            \frac{\partial{x}}{\partial{r}} & \frac{\partial{x}}{\partial{\theta}} \\
            \frac{\partial{y}}{\partial{r}} & \frac{\partial{y}}{\partial{\theta}} 
        \end{matrix}
    \right)
    = 
    \left(
        \begin{matrix}
            \cos{\theta} & -r\sin{\theta} \\
            \sin{\theta} & r\cos{\theta} 
        \end{matrix}
    \right) \\
\end{equation}

\begin{equation}
% ヤコビアン
    |\det{J}| = r \cos^2{\theta} + r \sin^2{\theta} \\
    = r(\cos^2{\theta} + \sin^2{\theta}) = r
\end{equation}

したがって、同時分布は

\begin{equation}
% rと\thetaの同時分布
    f(r, \theta) = f_X(r \cos{\theta})f_Y(r \sin{\theta})r = r c^2 \exp{\{- \frac{r^2}{2}\}}
\end{equation}
こいつを$r, \theta$について積分すると、実質$r$についての積分なので

$$
c^2 \int_0^{\infty} r \exp{\{- \frac{r^2}{2}\}} dr = 1
$$

これを解くと$\frac{1}{\sqrt{2\pi}}$になる。

#### ガンマ分布

## 条件付き期待値
世界一難しい


## 主な分布

### 多項分布
わからん


### 多変量正規分布
やさしい

 