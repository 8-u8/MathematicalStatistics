[TOP](https://8-u8.github.io/MathematicalStatistics/)

竹村数理統計の4章は全部別の人担当だったので、それは一旦飛ばす  
(ちゃんと自分非担当分も文字に起こさないとなあ)


# 数理統計学ノート05 統計的決定理論の枠組み

統計的決定理論は、推定(7章)、検定(8章)といった統計的推測を統一的に論じるために、Wald(1950)が導入した「考え方」
検定と推定を統一的に論じるために、抽象度の高い議論をする。とっつきにくいけどうまくやることで見通しが良くなる

## 用語と定義

### 標本空間・パラメータ空間
> 統計的推測は標本$X=(X_1, ..., X_n)$に基づいて行われる。

4章の冒頭にあるように、統計的推測とは、得られたデータに基づいて母集団について何かしらの結論を得ること
定義：標本空間$\mathscr{X}$とは、標本$X$の実現値が属する集合である

$n$次元の確率変数$X=(X_1, ..., X_n)$の場合、標本空間$\mathscr{X}$は$n$次元ユークリッド空間$\mathbb{R}^n$を取れば良い←なぜ？

標本空間として何を考えるかはどの確率変数に注目するかにも依存する。  
$n$個のベルヌーイ変数の場合標本空間は$\mathbb{R}^n$だが、成功数$Y=\sum_{i=1}^nX_i$二注目すれば、標本空間は$\mathbb{R}$で良い(1次元実数でOK)。

統計的推測においては$X \in \mathscr{X}$の従う分布族＝統計的モデルを想定する←なにそれ？

【例】$X_i \sim N(\mu, \sigma^2), i = 1,2,...,n,  i.i.d$

このとき、分布族は$\{N(\mu, \sigma^2) | -\infty < \mu < \infty, 0 < \sigma^2 < \infty\}$

内包的記法めいた表記だが「$\mu$と$\sigma^2$がそれぞれこの範囲で定義される正規分布$N(\mu, \sigma^2)$全体の集合」と解釈できる。

これをさらに一般化し、パラメータ$\theta$を持つ分布を$P_\theta$とおく。パラメータの取りうる値の集合を$\Theta$とすると、これをパラメータ空間(母数空間)と呼ぶ。  
正規分布を例に取ると、パラメータ空間は$\Theta = \{(\mu, \sigma^2) | -\infty < \mu < \infty, 0 < \sigma^2 < \infty\}$となる。

【練習】二項分布の場合の記述（p92）についてなぜそう言えるか検討せよ。

パラメータは多くの場合「未知」である（未知パラメータ）。だが、実務上既知とみなして推定に必要な情報量を減らすとかはよくやる。この場合「既知母数」と呼ぶ

### 統計家と決定空間、そして損失    

定義：統計家(Statistician)  
統計的な手法を用いて未知母数について何らかの判断を行う人。

定義：決定(Decision) $d$  
統計家のおこなう判断あるいは行動

【例】統計的仮説検定  
統計的検定は、帰無仮説(または対立仮説)を「棄却」するか、「受容」するかの2つの「決定」が考えられる。

定義：決定空間 $D$  
決定$d$の集合

【例】統計的仮説検定  
統計的仮説検定における決定空間$D = \{棄却, 受容\}$

【例】点推定
点推定の場合、統計家の決定は$\hat{\theta}$を良く推定することであり、決定空間とパラメータ空間$\Theta$は同値$D=\Theta$。

定義：損失関数$L(\theta, d)$  
真のパラメータが$\theta$であり、統計家が特定の決定$d$を行ったときに、  
統計家が被る損失の大きさを表す関数。  
**機械学習等における損失関数とは英語まで一緒だが、概念的に異なる点に注意**  
ここでの損失関数はパラメータとパラメータ真値との間に生まれる損失関数だが、  
機械学習は予測値と真値の間に生まれる損失関数である。
統計的仮説検定、統計的推定の目的は、統計家にとって、平均的に損失の少ない決定を行うことであり、この点で統一的に「決定理論」としての枠組みで議論することができる。

点推定の例：(5.1)式のような二乗誤差、絶対誤差などで、推定値と真の値の乖離を評価できる。

検定の例：(5.2)式のように、決定空間$D=\{棄却, 受容\}$を実数空間$\{0, 1\}$に写す写像を(脳内で)考えて、0-1損失によって評価すると良い

統計家は決定$d$を観測値(標本)$X$に基づいて選ぶので、決定$d$は$X$の関数$\delta(X)$で表現できる。

定義：決定関数、または決定方式$\delta(X)$  
標本空間から決定空間に写す写像  

$\delta: \mathscr{X} \rightarrow D$  

のこと。

決定関数は(標本$X$における)すべての実現値$x$に対して取るべき決定$d = \delta(x)$を定めるような関数である。

> 上は標本$X$ということにしたけど、これ標本空間$\mathscr{X}$なのかな。

【例】標本平均を決定関数として表せ。  
標本平均を$\bar{x}$とおくと、$\delta(x) = \bar{x}$

定義：リスク関数  
$\theta, \delta$を引数に持つ$R(\theta, \delta)$がリスク関数であるとは、  
$R(\theta, \delta)$が損失関数の期待値であること

【例】平均二乗誤差  
$R(\theta, \delta) = E_{\theta}[(\theta - \delta(X))^2]$

【練習】0-1損失のときのリスク関数を定義せよ

### 先に進む前に：頑健性
統計的決定理論では、決定関数の良し悪しをリスクの大小で比較するので、リスクの小さい決定関数が望ましい決定関数である。リスクはこの場合、リスク関数で評価する。リスク関数を使って望ましい決定関数を求める問題を統計的決定問題という。

統計的推測では、決定関数の平均的な損失で評価される。ここで注意したいのは「平均的な損失」が、必ずしもここの問題での良さを保証するものとは限らない。

リスク関数は、損失関数の選び方と母集団分布の仮定に依存する。  
【例】最小二乗法が望ましい場合の損失関数と母集団分布

こうした仮定の有無で、統計的決定とリスク評価に一貫性を持てるときは「頑健である」といい、  
仮定の有無による推定方式の頑健さの度合いを「頑健性」という。

## 許容性
統計的決定問題では、リスク関数を用いて決定方式を評価する。  
決定方式$\delta_1, \delta_2$があり、リスク関数$R(\theta, \delta_i), i=1,2$を定義する場面を考える。  
リスク関数を$\theta$の関数として考えてみると(p96、図5.1と図5.2)、真のパラメータ$\theta$が具体的にどんな値を取っているかによって、$\delta_1$と$\delta_2$のどちらが望ましいかは変わってくる。**リスク関数を最小にするような決定方式は、すべての$\theta$について一意の決定方式$\hat{\theta}$が決まるわけではない**。

リスク関数を最小にするような決定関数が一意に決まらないとき、我々には2つの道がある

1. リスク関数を比較する。比較をするための新たな基準を導入する→ミニマックス基準(5.3節)
2. 考察の対象とする決定関数のクラスを制限し、その制限されたクラスの中で、リスクを一様に最小化する決定方式を求める。→不偏推定量、ネイマン・ピアソンの補題(7章、8章)

【定義】「よい」あるいは「優越する」  
2つの決定方式$\delta_1, \delta_2$があり、「$\delta_1$が$\delta_2$よりよい(優越する)」とは、  
以下の2つが成立することである。

\begin{equation}
    \begin{aligned}
    R(\theta.\delta_1) \leq R(\theta, \delta_2)\  for \ all \ \theta \\
    R(\theta_0, \delta_1) < R(\theta_0, \delta_2)\ for \ some \ \theta_0 \ exists
    \end{aligned}
\end{equation}

【練習】上記の数式を日本語でいうとどうなる？？

「$\delta_1$が$\delta_2$よりよい(優越する)」ということを$\delta_1 \succ \delta_2$とかく。
等号を許す場合、$\delta_1 \succeq \delta_2$とも表現される。

ある決定方式$\delta$に対し、それを優越する決定方式$\delta^*$が存在するなら、統計的決定問題の目的に照らせば$\delta$を選択するのは合理的ではない。このときの$\delta$は非許容的と呼ばれる。  
逆に$\delta$が許容的であるとは、$\delta^* \succ \delta$を満たすような$\delta^*$が存在しないことをいう。  
許容性の定義は色々な形で表現できる(問5.1)。  

> 問5.1を示すことができれば、$R(\theta, \tilde{\delta}) \leq R(\theta, \delta) \Rightarrow R(\theta, \tilde{\delta}) = R(\theta, \delta)$ が許容性の定義として扱える（同値）。

ある決定方式$\delta$が許容的であるならば、特定の値$\theta_0$で$\delta$よりリスクの低い決定方式$\tilde{\delta}$は、必ずどこか別の値$\theta_1$で$\delta$よりもリスクが大きくなければならない。

> これ意味わかる？今の私の解釈だと、上記の優越の定義の2つ目の式の話だと思ってる。

許容性の概念は、最適性としては「弱い」。  
【例】正規分布に従う$X$を1つ観測して、そこからパラメータ $\mu$ を推定する。  
決定方式を、$\delta_0(X) \equiv 0$を満たすような$\delta_0$として設定すると、  
これは観測されるデータに関わらず、推定値を0にするような決定方式なので、統計的推測というよりは  
普通にエイヤな決め打ち。でもこれは「許容的」。

$\delta_0$のリスク関数を平均二乗誤差として考えると、

$$
R(\theta, \delta_0) = E[(\theta-0)^2] = \theta^2
$$

$\delta_0$が非許容的であると仮定すると、$\delta^* \succ \delta_0$を満たすような$\delta^*$が存在する、すなわちすべての$\theta$に対して $R(\theta.\delta^*) \leq R(\theta, \delta_0)$ となるはず。  
特に、$\theta=0$のとき、リスク関数は$\theta=0$と条件付けられた条件付き期待値である。つまり   

$$
R(0, \delta^*) = E_{\theta=0}[\delta^*(X)^2] = 0
$$

が成り立つ必要があるが、決定関数の二乗は0以上$\delta^*(X)^2 \geq 0$になるので、  
$P(\delta^*(X)=0 | \theta=0)=1$でなければならない。  
$\theta=0$のもとで$X$の密度関数の取りうる値は$\mathbb{R}^+$なので、そこから1つデータを持ってきて$\theta = \mu$を推定する場合、ほとんどすべての$x \in X$で$\delta^*(x)=0$とならなければならない。  
これはつまり$\delta^*$のほとんどいたるところで$\delta_0$に一致することと解釈できるが、これは$\delta^*$が$\delta_0$を優越する決定方式であり、$\delta_0$が非許容的であるという仮定に矛盾するため、$\delta_0$は許容的である。

つまりエイヤでいい感じに当て推量した値でも、「なんかいい推定量」になるし、それに基づいた意思決定は許容性という観点から見たら合理的になってしまう、ということになる。我々は実務で半ばやむを得ずこの決定をしていることもあるだろうなあ。

Steinのパラドックスという、「直観的に合理的である推定量が、じつは非許容的である(より良い推定量が存在する)」ことを厳密に数理的に証明した現象がある。

>Steinのパラドックスは縮小推定の話で、久保川本にも紙面が割かれている。ざっくりいうと「特定のセグメントに分けられる標本について、セグメント別の平均を取るよりも、全体平均方向に一定のバイアスがかかるように平均を取ったほうが、セグメント別の真の平均に対するより良い推定量となる」というお話。マルチレベルモデルなどにも応用されるので結構楽しい話ではある